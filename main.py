DATASET_CATEGORIES = {
    "Medical & Healthcare": {
        "D1": "Heart Disease (Comprehensive)",
        "D2": "Heart attack possibility",
        "D3": "Heart Disease Dataset",
        "D4": "Liver Disorders",
        "D5": "Diabetes Prediction",
        "D9": "Chronic Kidney Disease",
        "D10": "Breast Cancer Prediction",
        "D11": "Stroke Prediction",
        "D12": "Lung Cancer Prediction",
        "D13": "Hepatitis",
        "D15": "Thyroid Disease",
        "D16": "Heart Failure Prediction",
        "D17": "Parkinson's",
        "D18": "Indian Liver Patient",
        "D19": "COVID-19 Effect on Liver Cancer",
        "D20": "Liver Dataset",
        "D21": "Specht Heart",
        "D22": "Early-stage Diabetes",
        "D23": "Diabetic Retinopathy",
        "D24": "Breast Cancer Coimbra",
        "D25": "Chronic Kidney Disease",
        "D26": "Kidney Stone",
        "D28": "Echocardiogram",
        "D29": "Bladder Cancer Recurrence",
        "D31": "Prostate Cancer",
        "D46": "Real Breast Cancer Data",
        "D47": "Breast Cancer (Royston)",
        "D48": "Lung Cancer Dataset",
        "D52": "Cervical Cancer Risk",
        "D53": "Breast Cancer Wisconsin",
        "D61": "Breast Cancer Prediction",
        "D62": "Thyroid Disease",
        "D68": "Lung Cancer",
        "D69": "Cancer Patients Data",
        "D70": "Labor Relations",
        "D71": "Glioma Grading",
        "D74": "Post-Operative Patient",
        "D80": "Heart Rate Stress Monitoring",
        "D82": "Diabetes 2019",
        "D87": "Personal Heart Disease Indicators",
        "D92": "Heart Disease (Logistic)",
        "D95": "Diabetes Prediction",
        "D97": "Cardiovascular Disease",
        "D98": "Diabetes 130 US Hospitals",
        "D99": "Heart Disease Dataset",
        "D181": "HCV Data",
        "D184": "Cardiotocography",
        "D189": "Mammographic Mass",
        "D199": "Easiest Diabetes",
        "D200": "Monkey-Pox Patients",
        "D54": "Breast Cancer Wisconsin",
        "D63": "Sick-euthyroid",
        "D64": "Ann-test",
        "D65": "Ann-train",
        "D66": "Hypothyroid",
        "D67": "New-thyroid",
        "D72": "Glioma Grading",
    },

    "Gaming & Sports": {
        "D27": "Chess King-Rook",
        "D36": "Tic-Tac-Toe",
        "D40": "IPL 2022 Matches",
        "D41": "League of Legends",
        "D55": "League of Legends Diamond",
        "D56": "Chess Game Dataset",
        "D57": "Game of Thrones",
        "D73": "Connect-4",
        "D75": "FIFA 2018",
        "D76": "Dota 2 Matches",
        "D77": "IPL Match Analysis",
        "D78": "CS:GO Professional",
        "D79": "IPL 2008-2022",
        "D114": "Video Games",
        "D115": "Video Games Sales",
        "D117": "Sacred Games",
        "D118": "PC Games Sales",
        "D119": "Popular Video Games",
        "D120": "Olympic Games 2021",
        "D121": "Video Games ESRB",
        "D122": "Top Play Store Games",
        "D123": "Steam Games",
        "D124": "PS4 Games",
        "D116": "Video Games Sales",
    },

    "Education & Students": {
        "D43": "Student Marks",
        "D44": "Student 2nd Year Result",
        "D45": "Student Mat Pass/Fail",
        "D103": "Academic Performance",
        "D104": "Student Academic Analysis",
        "D105": "Student Dropout Prediction",
        "D106": "Electronic Gadgets Impact",
        "D107": "Campus Recruitment",
        "D108": "End-Semester Performance",
        "D109": "Fitbits and Grades",
        "D110": "Student Time Management",
        "D111": "Student Feedback",
        "D112": "Depression & Performance",
        "D113": "University Rankings",
        "D126": "University Ranking CWUR",
        "D127": "University Ranking CWUR 2013-2014",
        "D128": "University Ranking CWUR 2014-2015",
        "D129": "University Ranking CWUR 2015-2016",
        "D130": "University Ranking CWUR 2016-2017",
        "D131": "University Ranking CWUR 2017-2018",
        "D132": "University Ranking CWUR 2018-2019",
        "D133": "University Ranking CWUR 2019-2020",
        "D134": "University Ranking CWUR 2020-2021",
        "D135": "University Ranking CWUR 2021-2022",
        "D136": "University Ranking CWUR 2022-2023",
        "D137": "University Ranking GM 2016",
        "D138": "University Ranking GM 2017",
        "D139": "University Ranking GM 2018",
        "D140": "University Ranking GM 2019",
        "D141": "University Ranking GM 2020",
        "D142": "University Ranking GM 2021",
        "D143": "University Ranking GM 2022",
        "D144": "University Ranking Webometric 2012",
        "D145": "University Ranking Webometric 2013",
        "D146": "University Ranking Webometric 2014",
        "D147": "University Ranking Webometric 2015",
        "D148": "University Ranking Webometric 2016",
        "D149": "University Ranking Webometric 2017",
        "D150": "University Ranking Webometric 2018",
        "D151": "University Ranking Webometric 2019",
        "D152": "University Ranking Webometric 2020",
        "D153": "University Ranking Webometric 2021",
        "D154": "University Ranking Webometric 2022",
        "D155": "University Ranking Webometric 2023",
        "D156": "University Ranking URAP 2018-2019",
        "D157": "University Ranking URAP 2019-2020",
        "D158": "University Ranking URAP 2020-2021",
        "D159": "University Ranking URAP 2021-2022",
        "D160": "University Ranking URAP 2022-2023",
        "D161": "University Ranking THE 2011",
        "D162": "University Ranking THE 2012",
        "D163": "University Ranking THE 2013",
        "D164": "University Ranking THE 2014",
        "D165": "University Ranking THE 2015",
        "D166": "University Ranking THE 2016",
        "D167": "University Ranking THE 2017",
        "D168": "University Ranking THE 2018",
        "D169": "University Ranking THE 2019",
        "D170": "University Ranking THE 2020",
        "D171": "University Ranking THE 2021",
        "D172": "University Ranking THE 2022",
        "D173": "University Ranking THE 2023",
        "D174": "University Ranking QS 2022",
        "D190": "Student Academics Performance"
    },

    "Banking & Finance": {
        "D6": "Bank Marketing 1",
        "D7": "Bank Marketing 2",
        "D30": "Adult Income",
        "D32": "Telco Customer Churn",
        "D35": "Credit Approval",
        "D50": "Term Deposit Prediction",
        "D96": "Credit Card Fraud",
        "D188": "South German Credit",
        "D193": "Credit Risk Classification",
        "D195": "Credit Score Classification",
        "D196": "Banking Classification"
    },

    "Science & Engineering": {
        "D8": "Mushroom",
        "D14": "Ionosphere",
        "D33": "EEG Eye State",
        "D37": "Steel Plates Faults",
        "D39": "Fertility",
        "D51": "Darwin",
        "D58": "EEG Emotions",
        "D81": "Predictive Maintenance",
        "D84": "Oranges vs Grapefruit",
        "D90": "Crystal System Li-ion",
        "D183": "Drug Consumption",
        "D49": "Air Pressure System Failures",
        "D93": "Air Pressure System Failures",
        "D185": "Toxicity",
        "D186": "Toxicity",
    },

    "Social & Lifestyle": {
        "D38": "Online Shoppers",
        "D59": "Red Wine Quality",
        "D60": "White Wine Quality",
        "D88": "Airline Passenger Satisfaction",
        "D94": "Go Emotions Google",
        "D100": "Spotify East Asian",
        "D125": "Suicide Rates",
        "D182": "Obesity Levels",
        "D187": "Blood Transfusion",
        "D191": "Obesity Classification",
        "D192": "Gender Classification",
        "D194": "Happiness Classification",
        "D42": "Airline customer Holiday Booking dataset"
    },

    "ML Benchmarks & Synthetic": {
        "D34": "Spambase",
        "D85": "Synthetic Binary",
        "D89": "Naive Bayes Data",
        "D175": "Monk's Problems 1",
        "D176": "Monk's Problems 2",
        "D177": "Monk's Problems 3",
        "D178": "Monk's Problems 4",
        "D179": "Monk's Problems 5",
        "D180": "Monk's Problems 6"
    },

    "Other": {
        "D83": "Paris Housing",
        "D91": "Fake Bills",
        "D197": "Star Classification"
    }
}

import pandas as pd
#df = pd.read_csv('/content/drive/MyDrive/PLOS_dataset/classification_datasets.csv')
df = pd.read_csv('PLOS_dataset/classification_datasets.csv')
df.head()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import warnings
warnings.filterwarnings('ignore')

# Constants
DATASETS_FOLDER = "datasets"
RESULTS_FILE = "model_results.csv"

# ============================================================================
# LOAD ONE DATASET FUNCTION
# ============================================================================

def load_one_dataset(dataset_info, r=False):
    data_info = dataset_info
    """
    Load a dataset and split it into features (X) and target (y)
    """
    # Step 1: Extract information from dataset_info
    dataset_name = data_info['dataset']
    task_type = data_info['task_type']
    main_file = data_info['main_file']
    target_column = data_info['target_column']

    # Step 2: Print header
    print(f"\n{'='*50}")
    print(f"Dataset: {dataset_name}")
    print(f"Task: Classification")
    print(f"Target: {target_column}")
    print('='*50)
    # Step 1: Build the file path using f-string
    file_path = f"PLOS_dataset/datasets/{dataset_name}/{main_file}"

    # Step 3: Read the CSV file with encoding fallback
    try:
        if main_file.endswith('.csv'):
            # Try reading CSV with encoding fallback
            try:
                data = pd.read_csv(file_path, sep=None, engine='python')
            except UnicodeDecodeError:
                data = pd.read_csv(file_path, sep=None, engine='python', encoding='latin-1')
                print(f"Used latin-1 encoding for {dataset_name}")
        elif main_file.endswith(('.xlsx', '.xls')):
            # Read Excel file
            data = pd.read_excel(file_path)
        else:
            print(f"Unknown file type: {main_file}")
            return None, None

    except Exception as e:
        print(f"Error reading {dataset_name}: {e}")
        return None, None

    print(f"Loaded {data.shape[0]} rows, {data.shape[1]} columns")

    # Step 4: Split into X and y
    X = data.drop(columns=[target_column])
    y = data[target_column]

    # Step 5: Print summary info
    print(f"Features: {X.shape[1]}, Target classes: {y.nunique()}")

    # Step 6: Return the data
    if r:
      return data.shape[0]
    return X,y



def preprocess_one_data(X, y):
    """
    Clean and prepare the data for machine learning
    Steps:
    1. Fill missing values (mean for numeric, mode for categorical)
    2. Encode categorical columns using Label Encoding/ One Hot Encoding
    Args:
        X: Feature data
        y: Target data
    Returns:
        Preprocessed versions of all four datasets
    """
    if X is None or y is None:
        return None, None, None, None

    X_train_clean = X.copy()
    y_train_clean = y.copy()
    X_train, X_test, y_train, y_test = train_test_split(X_train_clean, y_train_clean, test_size=0.2, random_state=42)

    categorical_cols = X_train.select_dtypes(include='object').columns
    numeric_cols = X_train.select_dtypes(include=np.number).columns

    # Handle numeric columns
    for col in numeric_cols:
        mean_value = X_train[col].mean()
        X_train[col] = X_train[col].fillna(mean_value)
        X_test[col] = X_test[col].fillna(mean_value)
        X_train[col] = X_train[col].fillna(0) # Fill any remaining NaNs with 0
        X_test[col] = X_test[col].fillna(0) # Fill any remaining NaNs with 0


    # Handle categorical columns
    for col in categorical_cols:
        mode_value = X_train[col].mode()[0] if not X_train[col].mode().empty else 'missing'
        X_train[col] = X_train[col].fillna(mode_value).astype(str)
        X_test[col] = X_test[col].fillna(mode_value).astype(str)

        if X_train[col].nunique() < 10:
            # One-Hot Encode
            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
            X_train_encoded = ohe.fit_transform(X_train[[col]])
            X_test_encoded = ohe.transform(X_test[[col]])

            # Create new column names for one-hot encoded features
            new_col_names = [f"{col}_{cat}" for cat in ohe.categories_[0]]

            # Create DataFrames from encoded arrays
            X_train_encoded_df = pd.DataFrame(X_train_encoded, index=X_train.index, columns=new_col_names)
            X_test_encoded_df = pd.DataFrame(X_test_encoded, index=X_test.index, columns=new_col_names)

            # Drop the original categorical column and concatenate the new one
            X_train = X_train.drop(columns=[col])
            X_test = X_test.drop(columns=[col])
            X_train = pd.concat([X_train, X_train_encoded_df], axis=1)
            X_test = pd.concat([X_test, X_test_encoded_df], axis=1)

        else:
            # Label Encode
            le = LabelEncoder()
            X_train[col] = le.fit_transform(X_train[col])
            # Use a try-except block for transform to handle unseen labels
            try:
                X_test[col] = le.transform(X_test[col])
            except ValueError:
                # Handle unseen labels in test set - replace with a placeholder or mode
                # For simplicity, we'll replace unseen labels in the test set with the mode of the training set.
                unseen_labels = set(X_test[col].unique()) - set(le.classes_)
                for unseen_label in unseen_labels:
                    X_test[col] = X_test[col].replace(unseen_label, mode_value)
                X_test[col] = le.transform(X_test[col])



    if y_train.dtype == 'object':
        le = LabelEncoder()
        y_train = le.fit_transform(y_train)
        y_test = le.transform(y_test)

    # Ensure X_train and X_test have the same columns after one-hot encoding
    train_cols = X_train.columns
    test_cols = X_test.columns

    missing_in_test = set(train_cols) - set(test_cols)
    for c in missing_in_test:
        X_test[c] = 0

    missing_in_train = set(test_cols) - set(train_cols)
    for c in missing_in_train:
        X_train[c] = 0

    X_test = X_test[train_cols]


    print(f"Final feature count: {X_train.shape[1]}")
    return X_train, X_test, y_train, y_test



from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

def train_one_dataset_classification_models(X_train, X_test, y_train, y_test, models_to_run, enable_tuning=False, tuning_method='Grid Search', random_iter=10, cv_folds=5):
    if X_train is None or X_test is None or y_train is None or y_test is None:
      return 'no data'

    results = []
    models_dict = {
        'rf': ('RandomForest', RandomForestClassifier(random_state=42)),
        'knn': ('KNN', KNeighborsClassifier()),
        'dt': ('DecisionTree', DecisionTreeClassifier(random_state=42)),
        'svm': ('SVM', SVC(kernel='rbf', random_state=42)),
        'lr': ('LogisticRegression', LogisticRegression(random_state=42))
    }

    # Define parameter grids for tuning
    param_grid_rf = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5]
    }
    param_grid_knn = {
        'n_neighbors': [3, 5, 7, 9],
        'weights': ['uniform', 'distance']
    }
    param_grid_dt = {
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5],
        'criterion': ['gini', 'entropy']
    }
    param_grid_svm = {
        'C': [0.1, 1, 10],
        'gamma': ['scale', 'auto'],
        'kernel': ['rbf']
    }
    param_grid_lr = {
        'C': [0.1, 1, 10],
        'solver': ['liblinear', 'lbfgs']
    }

    param_grids = {
        'RandomForest': param_grid_rf,
        'KNN': param_grid_knn,
        'DecisionTree': param_grid_dt,
        'SVM': param_grid_svm,
        'LogisticRegression': param_grid_lr
    }

    for model_key in models_to_run:
        model_name, model_instance = models_dict[model_key]
        current_model = model_instance
        tuning_info = {}

        if enable_tuning and model_name in param_grids:
            print(f"Applying {tuning_method} for {model_name}...")
            param_grid = param_grids[model_name]

            if tuning_method == 'Grid Search':
                search = GridSearchCV(current_model, param_grid, cv=cv_folds, scoring='accuracy', n_jobs=-1)
            elif tuning_method == 'Random Search':
                search = RandomizedSearchCV(current_model, param_grid, n_iter=random_iter, cv=cv_folds, scoring='accuracy', n_jobs=-1, random_state=42)

            if search:
                search.fit(X_train, y_train)
                current_model = search.best_estimator_
                tuning_info = {
                    'tuned': True,
                    'tuning_method': tuning_method,
                    'best_params': str(search.best_params_),
                    'best_score_cv': np.round(search.best_score_, decimals=3)
                }
                print(f"Best parameters for {model_name}: {search.best_params_}")
                print(f"Best cross-validation score for {model_name}: {search.best_score_:.4f}")
        else:
            tuning_info = {'tuned': False}

        # Train the model (either original or best_estimator_ from tuning)
        current_model.fit(X_train, y_train)
        y_pred = current_model.predict(X_test)

        # Extract feature importances/coefficients
        feature_importances_data = None
        if hasattr(current_model, 'feature_importances_'):
            feature_importances_data = dict(zip(X_train.columns.tolist(), current_model.feature_importances_.tolist()))
        elif hasattr(current_model, 'coef_'):
            # For binary classification, coef_ is usually 1D or 2D where the first row is for the positive class
            if current_model.coef_.ndim > 1 and current_model.coef_.shape[0] > 1: # Multi-class LR
                # For multi-class, store each class's coefficients
                feature_importances_data = {
                    f'class_{i}': dict(zip(X_train.columns.tolist(), current_model.coef_[i].tolist()))
                    for i in range(current_model.coef_.shape[0])
                }
            else: # Binary classification LR
                feature_importances_data = dict(zip(X_train.columns.tolist(), current_model.coef_[0].tolist()))
        else:
            feature_importances_data = 'N/A'

        dictionary = {
            'model': model_name,
            'accuracy': np.round(accuracy_score(y_test, y_pred), decimals = 3),
            'precision':np.round(precision_score(y_test, y_pred, average='weighted', zero_division=0), decimals=3),
            'recall': np.round(recall_score(y_test, y_pred, average='weighted', zero_division=0), decimals = 3),
            'f1_score': np.round(f1_score(y_test, y_pred, average='weighted', zero_division=0), decimals = 3),
            'feature_importances': feature_importances_data,
            **tuning_info # Add tuning information to the results
        }
        results.append(dictionary)
        print(f"{model_name}: accuracy = {dictionary['accuracy']:.4f}% precision = {dictionary['precision']:.4f}% recall = {dictionary['recall']:.4f}% f1 = {dictionary['f1_score']:.4f}% {" (Tuned)" if tuning_info.get('tuned') else ''}")

    return results


def process_one_dataset(dataset_info, models = ['knn', 'svm', 'lr', 'dt', 'rf'], enable_tuning=False, tuning_method='Grid Search', random_iter=10, cv_folds=5):
  x,y = load_one_dataset(dataset_info)
  X_train, X_test, y_train, y_test = preprocess_one_data(x,y)
  print("Training classification models...")
  results = train_one_dataset_classification_models(X_train, X_test, y_train, y_test, models, enable_tuning, tuning_method, random_iter, cv_folds)
  #results is data = dataname, task_type = classification
  if results == 'no data':
    return 'no data'
    print('no data')
  for result in results:
    result['dataset'] = dataset_info['dataset']
    result['task_type'] = 'classification'
  return results

import pandas as pd

# Reload the original classification datasets DataFrame
df = pd.read_csv('PLOS_dataset/classification_datasets.csv')

!pip -q install gradio
import gradio as gr
#import pandas as pd
#import numpy as np
#for the gradio app, make selectable columns to input into the testing. add a randomized grid search or grid search as options. could also be cool to choose functions to compare


import pandas as pd
import plotly.express as px
import plotly.graph_objects as go # Added import for plotly.graph_objects
import seaborn as sns

def create_heatmaps(pivoted_dfs):
    figures = []
    metrics_order = ['accuracy', 'precision', 'recall', 'f1_score']
    # Changed from sns.cubehelix_palette to a plotly-compatible colorscale string
    cmap_name = "gray"

    for metric in metrics_order:
        pivot_df = pivoted_dfs.get(metric, pd.DataFrame())

        if not pivot_df.empty and not pivot_df.isnull().all().all():
            fig = px.imshow(
                pivot_df,
                color_continuous_scale=cmap_name,
                aspect="auto",
                text_auto=False
            )

            fig.update_traces(
                hovertemplate="<b>%{x}</b> vs <b>%{y}</b><br>Value: %{z:.2f}<extra></extra>",

            )
            fig.update_layout(
                title=f"{metric.capitalize()} Heatmap",
                margin=dict(l=0, r=0, t=40, b=0),
                coloraxis_colorbar=dict(title="", tickfont_color='white'), # Changed colorbar title
                paper_bgcolor="rgba(0,0,0,0)",
                plot_bgcolor="rgba(0,0,0,0)",
                xaxis_tickfont_color='white',
                yaxis_tickfont_color='white',
                title_font_color='white',
                xaxis_title_font_color='white',
                yaxis_title_font_color='white'
            )

            figures.append(fig)

        else:
            # Empty figure for missing data
            fig = px.imshow([[None]], text_auto=False)
            fig.update_layout(
                title=f"{metric.capitalize()} Heatmap (No data)",
                annotations=[dict(
                    text="No data to display",
                    x=0.5, y=0.5, xref="paper", yref="paper",
                    showarrow=False, font=dict(size=16, color='white')
                )],
                coloraxis_showscale=False,
                title_font_color='white'
            )
            figures.append(fig)

    return figures


import gradio as gr
import matplotlib.pyplot as plt
import seaborn as sns

def process_multiple_datasets(indices, models =  ['knn', 'svm', 'lr', 'dt', 'rf'], enable_tuning=False, tuning_method='Grid Search', random_iter=10, cv_folds=5):
    all_results = []
    all_feature_importances_data = [] # Initialize list for feature importances

    if 'all' in indices:
      for i in range(30):
        dataset_info = df.iloc[i]
        current_dataset_results = process_one_dataset(dataset_info, models, enable_tuning, tuning_method, random_iter, cv_folds)
        if current_dataset_results != 'no data':
            all_results.extend(current_dataset_results)


    elif 'beginner' in indices:
      for i in df[df['difficulty'] == 'beginner'].index:
        dataset_info = df.iloc[i]
        current_dataset_results = process_one_dataset(dataset_info, models, enable_tuning, tuning_method, random_iter, cv_folds)
        if current_dataset_results != 'no data':
            all_results.extend(current_dataset_results)


    elif 'intermediate' in indices:
      for i in df[df['difficulty'] == 'intermediate'].index:
        dataset_info = df.iloc[i]
        current_dataset_results = process_one_dataset(dataset_info, models, enable_tuning, tuning_method, random_iter, cv_folds)
        if current_dataset_results != 'no data':
            all_results.extend(current_dataset_results)

    else:
        for index in indices:
            dataset_info = df.iloc[index]
            current_dataset_results = process_one_dataset(dataset_info, models, enable_tuning, tuning_method, random_iter, cv_folds)
            if current_dataset_results != 'no data':
                all_results.extend(current_dataset_results)

    # Process feature importances
    for model_result in all_results:
        dataset_name = model_result['dataset']
        model_name = model_result['model']
        feature_importances = model_result.get('feature_importances') # Use .get to safely retrieve

        if feature_importances != 'N/A' and feature_importances is not None:
            # Check if it's a multi-class LR format
            if isinstance(feature_importances, dict) and any(key.startswith('class_') for key in feature_importances.keys()):
                for class_key, features_dict in feature_importances.items():
                    for feature, importance in features_dict.items():
                        all_feature_importances_data.append({
                            'dataset': dataset_name,
                            'model': model_name,
                            'feature': f"{feature}_{class_key}", # Append class info to feature name
                            'importance': importance
                        })
            elif isinstance(feature_importances, dict):
                # Single-output (e.g., Tree-based, Binary LR)
                for feature, importance in feature_importances.items():
                    all_feature_importances_data.append({
                        'dataset': dataset_name,
                        'model': model_name,
                        'feature': feature,
                        'importance': importance
                    })

    results_df = pd.DataFrame(all_results)
    # Reorder columns for better display in Gradio
    if not results_df.empty:
        new_order = ['dataset', 'model', 'accuracy', 'precision', 'recall', 'f1_score']
        current_cols = results_df.columns.tolist()
        ordered_cols = [col for col in new_order if col in current_cols]
        results_df = results_df[ordered_cols]

    feature_importances_df = pd.DataFrame(all_feature_importances_data)

    pivoted_dfs = {}
    metrics = ['accuracy', 'precision', 'recall', 'f1_score']
    for metric in metrics:
        if not results_df.empty:
            pivot_df = results_df.pivot_table(index='model', columns='dataset', values=metric)
            pivoted_dfs[metric] = pivot_df
        else:
            pivoted_dfs[metric] = pd.DataFrame()

    heatmap_figures = create_heatmaps(pivoted_dfs)

    # Ensure we return exactly four figures, even if some are empty plots
    # This assumes create_heatmaps always attempts to return 4 figures (or empty plots).

    # If no data is processed, return empty results_df and None for figures
    if results_df.empty:
        return results_df, None, None, None, None, pd.DataFrame()

    # Make sure we return 4 figures. If fewer are generated, fill with None. If more, take the first 4.
    output_figures = heatmap_figures + [None] * (4 - len(heatmap_figures))
    output_figures = output_figures[:4]

    return results_df, output_figures[0], output_figures[1], output_figures[2], output_figures[3], feature_importances_df


import pandas as pd
from scipy.stats import ttest_rel
def statistical_analysis(indices, tree_models = ["rf", "dt"], non_tree_models = ["knn", "svm", "lr"], enable_tuning=False, tuning_method='Grid Search', random_iter=10, cv_folds=5):
  #tree_models = ["RandomForest", "DecisionTree"]
  #non_tree_models = ["KNN", "SVM", "LogisticRegression"]
  models = tree_models
  for item in non_tree_models:
    models.append(item)
  newtree = []
  new_nontree = []
  if 'rf' in tree_models:
    newtree.append('RandomForest')
  if 'knn' in non_tree_models:
    new_nontree.append('KNN')
  if 'dt' in tree_models:
    newtree.append('DecisionTree')
  if 'svm' in non_tree_models:
    new_nontree.append('SVM')
  if 'lr' in non_tree_models:
    new_nontree.append('LogisticRegression')
  non_tree_models = new_nontree
  tree_models = newtree

  # Call process_multiple_datasets to get model metrics, heatmaps, and feature importances
  model_metrics_df, acc_heatmap, prec_heatmap, rec_heatmap, f1_heatmap, feature_importances_df = process_multiple_datasets(indices, models, enable_tuning, tuning_method, random_iter, cv_folds)

  # Store all results
  all_statistical_results = [] # Renamed to avoid conflict with model_metrics_df
  # For each metric
  for metric in ["accuracy", "precision", "recall", "f1_score"]:

      comparison_num = 1

      for tree_model in tree_models:
          for non_tree_model in non_tree_models:

              tree_data = model_metrics_df[model_metrics_df['model'] == tree_model].set_index('dataset')[metric]
              non_tree_data = model_metrics_df[model_metrics_df['model'] == non_tree_model].set_index('dataset')[metric]
              # Align datasets (inner join - only datasets present for both models)
              combined = pd.DataFrame({
                  'tree': tree_data,
                  'non_tree': non_tree_data
              }).dropna()
              print(combined)
              if len(combined) < 2:
                  print(f"{comparison_num:<3} {tree_model:<20} {non_tree_model:<20} Insufficient data")
                  comparison_num += 1
                  continue

              # Paired t-test
              t_stat, p_val = ttest_rel(combined['tree'], combined['non_tree'])

              # Calculate means and stds
              mean1 = combined['tree'].mean()
              mean2 = combined['non_tree'].mean()
              std1 = combined['tree'].std()
              std2 = combined['non_tree'].std()
              n = len(combined)

              sig = "< 0.001" if p_val < 0.001 else f"{p_val:.3f}"

              print(f"{comparison_num:<3} {tree_model:<20} {non_tree_model:<20} {mean1:<10.5f} {mean2:<10.5f} {t_stat:<8.2f} {sig:<10} {'True' if p_val < 0.05 else 'False'}")

              all_statistical_results.append({
                  'metric': metric,
                  'tree_model': tree_model,
                  'non_tree_model': non_tree_model,
                  'p_value': np.round(p_val, decimals = 3),
                  'tree_mean': np.round(mean1, decimals = 3),
                  'non_tree_mean': np.round(mean2, decimals = 3),
                  'tree_std': np.round(std1, decimals = 3),
                  'non_tree_std': np.round(std2, decimals = 3),
                  'n_datasets': n,
                  't_statistic': np.round(t_stat, decimals = 3)
              })

              comparison_num += 1
  statistical_results_df = pd.DataFrame(all_statistical_results) # Renamed this dataframe
  significant_count = (statistical_results_df['p_value'] < 0.05).sum()
  total_count = len(statistical_results_df)
  print(f"Tree models won in: {(statistical_results_df['tree_mean'] > statistical_results_df['non_tree_mean']).sum()} comparisons")

  # Save detailed results
  statistical_results_df.to_csv('pairwise_comparison_results.csv', index=False)

  # Return all necessary dataframes and plots for Gradio interface
  return model_metrics_df, acc_heatmap, prec_heatmap, rec_heatmap, f1_heatmap, feature_importances_df, statistical_results_df, str(np.round((statistical_results_df['p_value'] < 0.05).sum()/len(statistical_results_df), decimals = 3))


import pandas as pd
from scipy.stats import ttest_rel

def combined_statistical_analysis(
    medical_datasets_val, gaming_sports_datasets_val, education_students_datasets_val,
    banking_finance_datasets_val, science_engineering_datasets_val, social_lifestyle_datasets_val,
    ml_benchmarks_synthetic_datasets_val, other_datasets_val,
    difficulty_datasets_val,
    tree_models_selection_val, non_tree_models_selection_val,
    enable_tuning, tuning_method, random_iter, cv_folds
):
    all_selected_dataset_ids = []
    # Collect all selected dataset IDs from all dataset-related dropdowns
    for selection_list in [
        medical_datasets_val, gaming_sports_datasets_val, education_students_datasets_val,
        banking_finance_datasets_val, science_engineering_datasets_val, social_lifestyle_datasets_val,
        ml_benchmarks_synthetic_datasets_val, other_datasets_val, difficulty_datasets_val
    ]:
        if selection_list: # selection_list will be a list of strings (e.g., ['D1', 'D5'])
            all_selected_dataset_ids.extend(selection_list)

    final_indices = []
    # Handle special selections ('all', 'beginner', 'intermediate')
    if 'all' in all_selected_dataset_ids:
        final_indices = ['all']
    elif 'beginner' in all_selected_dataset_ids:
        final_indices = ['beginner']
    elif 'intermediate' in all_selected_dataset_ids:
        final_indices = ['intermediate']
    else:
        # Convert dataset IDs (like 'D1') to integer indices for df.iloc[i]
        global df
        if 'df' not in globals():
            df = pd.read_csv('PLOS_dataset/classification_datasets.csv')

        dataset_id_to_index_map = {row['dataset']: idx for idx, row in df.iterrows()}
        for ds_id in all_selected_dataset_ids:
            # Ensure we are handling potential labels from dropdowns if they are in the format "D1: Dataset Name"
            actual_ds_id = ds_id.split(':')[0].strip() if ':' in ds_id else ds_id
            if actual_ds_id in dataset_id_to_index_map:
                final_indices.append(dataset_id_to_index_map[actual_ds_id])
            else:
                print(f"Warning: Dataset ID '{actual_ds_id}' not found in df. Ignoring.")

    final_indices = list(set(final_indices)) # Remove duplicates

    # Call the existing statistical_analysis function
    return statistical_analysis(final_indices, tree_models_selection_val, non_tree_models_selection_val, enable_tuning, tuning_method, random_iter, cv_folds)


treemodel_choices = ['rf', 'dt']
nontreemodel_choices = ['knn','svm','lr']

dataset_inputs = []
for category_name, datasets_dict in DATASET_CATEGORIES.items():
    choices_for_dropdown = []
    for dataset_id, dataset_name in datasets_dict.items():
        choices_for_dropdown.append((f"{dataset_id}: {dataset_name}", dataset_id))
    dataset_inputs.append(gr.Dropdown(choices=choices_for_dropdown, label=f"{category_name} Datasets", multiselect=True))

# Add a dropdown for special dataset selections (all, beginner, intermediate)
special_dataset_choices = [
    ("All Datasets (first 30)", "all"),
    ("Beginner Datasets", "beginner"),
    ("Intermediate Datasets", "intermediate")
]
dataset_inputs.append(gr.Dropdown(choices=special_dataset_choices, label="Difficulty-based Datasets", multiselect=True))

# Add new tuning options
enable_tuning_checkbox = gr.Checkbox(value=False, label='Enable Hyperparameter Tuning')
tuning_method_dropdown = gr.Dropdown(choices=['Grid Search', 'Random Search'], value='Grid Search', label='Tuning Method')
random_iter_number = gr.Number(value=5, minimum=1, label='Random Search Iterations')
cv_folds_number = gr.Number(value=5, minimum=2, label='Cross-validation Folds')

# Combine all inputs for Gradio
all_gradio_inputs = dataset_inputs + [
    gr.Dropdown(treemodel_choices, label="Select Tree Based Models", multiselect=True, value=['rf', 'dt']),
    gr.Dropdown(nontreemodel_choices, label="Select Non Tree Based Models", multiselect=True, value=['knn', 'svm', 'lr']),
    enable_tuning_checkbox,
    tuning_method_dropdown,
    random_iter_number,
    cv_folds_number
]

demo = gr.Interface(
    fn=combined_statistical_analysis,
    inputs=all_gradio_inputs,
    outputs=[
        gr.Dataframe(label="Model Results"),
        gr.Plot(label='Accuracy Heatmap'),
        gr.Plot(label='Precision Heatmap'),
        gr.Plot(label='Recall Heatmap'),
        gr.Plot(label='F1-Score Heatmap'),
        gr.Dataframe(label="Feature Importances"), # Added this line for feature importances
        gr.Dataframe(label="Statistical Analysis Results"),
        gr.Textbox(label = "Proportion of cases where tree based was statistically superior")
    ],
    title="Tree based VS Non tree based",
)

demo.launch(share=True, show_error=True)
